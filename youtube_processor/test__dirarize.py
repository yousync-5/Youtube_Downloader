# test_diarize.py
import json
from collections import defaultdict
import torch
from pyannote.audio import Pipeline
import os
from dotenv import load_dotenv

from pyannote.audio import Inference
from visualize import visualize_diarization


load_dotenv()  # .env íŒŒì¼ì˜ HF_TOKEN ë“±ì„ ë¡œë“œ

# í•˜ë“œì½”ë”©í•œ íŒŒì¼ ê²½ë¡œ ì„¤ì •
VOCAL_PATH = "separated/htdemucs/4KYfTRe1pC4/vocals.wav"
SEGMENTS_JSON = "cached_data/post_word_data.json"


def test_diarization(vocal_path: str, segments_json: str):
    # 1) post_word_data ë¶ˆëŸ¬ì˜¤ê¸°
    with open(segments_json, 'r', encoding='utf-8') as f:
        post_word_data = json.load(f)

    # 2) pyannote íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"â–¶ï¸ Loading diarization model on {device}")
    pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1",
        use_auth_token=True
    )

    my_embedder = Inference(
        "pyannote/embedding",
        device=device,
        use_auth_token=True,
        duration=0.02,   #í•œë²ˆ ì„ë² ë”©ì„ ì¶”ì¶œí•  ì˜¤ë””ì˜¤ ê¸¸ì´(ì´ˆ) - ì‘ì„ ìˆ˜ë¡(ì§§ì€ êµ¬ê°„ ë‹¨ìœ„ ë””í…Œì¼)
        # ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì–¼ë§ˆë‚˜ ì´ë™í• ì§€ - ì‘ì„ ìˆ˜ë¡(ê²¹ì¹¨ ê²½ê³„ ê°ì§€ ì •ë°€) 
        step=0.02
    )
    # 3. íŒŒì´í”„ë¼ì¸ì— ë®ì–´ì“°ê¸°

    pipeline.embedding = my_embedder

    # ë¶„ì ˆ(segmentation) ìœˆë„ìš° í¬ê¸° & ì´ë™(step)
    # pipeline.segmentation.chunk_duration = 0.05   # ê¸´ ìœˆë„ìš°ë¡œ ì‘ì€ ì¡ìŒ ë¬´ì‹œ
    # pipeline.segmentation.step = 0.05  #ìœˆë„ìš°ë¥¼ ì–¼ë§ˆë‚˜ ìì£¼ ì›€ì§ì¼ì§€ (ê¸°ë³¸ 0.1â€‰s).
    
    # ìŒì„± íƒì§€(VAD) ë¯¼ê°ë„
    # ë°œí™” ì‹œì‘ ê°ì§€ ì„ê³„ì¹˜ - ì‘ì„ ìˆ˜ë¡(ë‚®ì€ ìŒëŸ‰ë„ ë°œí™”ë¡œ ê°„ì£¼), í´ ìˆ˜ë¡(í° ëª©ì†Œë¦¬ë§Œ ë°œí™”ë¡œ ì¸ì‹)
    pipeline.segmentation.onset  = 0.25
    # ë°œí™” ì¢…ë£Œ ê°ì§€ ì„ê³„ì¹˜ - ì‘ì„ ìˆ˜ë¡(ì§§ì€ ë¬´ìŒ êµ¬ê°„ë§Œìœ¼ë¡œë„ ëŠê¹€ ì²˜ë¦¬), í´ìˆ˜ë¡(ë” ê¸´ ë¬´ìŒ êµ¬ê°„ì´ ìˆì–´ì•¼ ë°œí™” ì¢…ë£Œë¡œ ê°„ì£¼)
    pipeline.segmentation.offset = 0.7

    # ìµœì†Œ ë°œí™”/ ì¹¨ë¬µ ê¸¸ì´
    # ìµœì†Œ ë°œí™” ê¸¸ì´ - ì‘ì„ ìˆ˜ë¡(ì§§ì€ ë‹¨ì–´, ìŒì ˆë„ ë°œí™” í• ë‹¹), í´ ìˆ˜ë¡(ë¹ˆë§, ì§§ì€ ì‚‘ì‚¬ë¦¬ ì œê±°)
    pipeline.min_duration_on  = 0.2
    # ìµœì†Œ ì¹¨ë¬µ ê¸¸ì´ - ì‘ì„ ìˆ˜ë¡(ì§§ì€ ë¬´ìŒì—ë„ ë°œí™”ê°€ ëŠê¸´ ê²ƒìœ¼ë¡œ ì²˜ë¦¬), í´ ìˆ˜ë¡(ë¬´ìŒ êµ¬ê°„ì´ ê¸¸ì–´ì•¼ ë°œí™” ì¤‘ë‹¨)
    pipeline.min_duration_off = 0.4

    # í´ëŸ¬ìŠ¤í„°ë§ ë°©ì‹ & íŒŒë¼ë¯¸í„°
    # í™”ì ì„ë² ë”© ê°„ ê±°ë¦¬(í˜¹ì€ ìœ ì‚¬ë„ ê¸°ì¤€)
    pipeline.clustering.threshold = 0.8

    # ìµœì†Œ í´ëŸ¬ìŠ¤í„° ìƒ˜í”Œ ìˆ˜ - ì‘ì„ ìˆ˜ë¡(ì§§ì€ ë°œí™”ë„ í´ëŸ¬ìŠ¤í„°ë¡œ ì¸ì •), í´ ìˆ˜ë¡(ì¶©ë¶„íˆ ìì£¼ ë“±ì¥í•œ ë°œí™”ë§Œ ì¸ì •)
    # pipeline.clustering.min_samples = 7


    pipeline.to(device)
    print("âœ… Model loaded.")


    # 3) í™”ì ë¶„ë¦¬ ì‹¤í–‰
    diarization = pipeline({"audio": vocal_path}, min_speakers=2, max_speakers=2)
    print("âœ… Diarization complete.")

    # ì‹œê°í™”
    visualize_diarization(vocal_path, diarization)



    # 4) ì„¸ê·¸ë¨¼íŠ¸ë³„ í™”ì í• ë‹¹
    speaker_segments = defaultdict(list)
    for seg in post_word_data:
        best_speaker, best_overlap = "unknown", 0.0
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            overlap = max(0.0,
                          min(turn.end, seg["end"]) -
                          max(turn.start, seg["start"]))
            if overlap > best_overlap:
                best_overlap, best_speaker = overlap, speaker
        seg["speaker_label"] = best_speaker
        speaker_segments[best_speaker].append(seg)
    
   
    print(f"Detected speakers: {list(speaker_segments.keys())}")

    # 5) ì£¼ìš” í™”ì ì¶”ì¶œ
    valid = {sp: segs for sp, segs in speaker_segments.items() if sp != "unknown"}
    if not valid:
        print("âŒ No valid speaker found.")
        return

    main_sp, segs = max(valid.items(), key=lambda item: sum(s["end"]-s["start"] for s in item[1]))
    print(f"ğŸ‘‘ Main speaker: {main_sp}")

    # 6) ì„¸ê·¸ë¨¼íŠ¸ ì •ë ¬ ë° ì¶œë ¥
    segs = sorted(segs, key=lambda x: x["start"])
    print("ğŸ“ Main speaker segments:")
    for i, s in enumerate(segs, 1):
        print(f"  [{i}] {s['start']:.2f}-{s['end']:.2f} | {s['text']}")

if __name__ == "__main__":
    # í•˜ë“œì½”ë”©ëœ íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸
    test_diarization(VOCAL_PATH, SEGMENTS_JSON)
