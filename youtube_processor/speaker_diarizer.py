
# speaker_diarizer.py
from collections import defaultdict
from typing import List, Dict, Any
from pathlib import Path
import torch
from pydub import AudioSegment
from pyannote.audio import Pipeline
from pyannote.core import Annotation
from pyannote.audio import Inference


def diarize_main_speaker(
    vocal_path: str,
    post_word_data: List[Dict[str, Any]],   # â† ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸!
    hf_token: str,
    *,                       # í‚¤ì›Œë“œ ì „ìš©
    min_speakers: int = 2,
    max_speakers: int = 2,
) -> Dict[str, Any]:
    """Whisper ì„¸ê·¸ë¨¼íŠ¸(post_word_data)ì— í™”ì ë¼ë²¨ì„ ë‹¬ê³ 
       ëŒ€ì‚¬ëŸ‰ì´ ê°€ì¥ ê¸´ í™”ìë¥¼ ë°˜í™˜í•œë‹¤."""

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. ì…ë ¥ WAV í™•ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    src = Path(vocal_path)
    if src.suffix.lower() != ".wav":            # MP3 â†’ WAV 16 kHz mono
        wav = src.with_suffix(".wav").with_name(f"{src.stem}_16k_mono.wav")
        if not wav.exists():
            print(f"ğŸ”„ {src.name} â†’ {wav.name} (16 kHz mono)")
            AudioSegment.from_file(src).set_frame_rate(16_000)\
                                        .set_channels(1)\
                                        .export(wav, format="wav")
        vocal_path = str(wav)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    print("ğŸ”Š diarization start")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    try:
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token,
            # í•„ìš”í•˜ë‹¤ë©´ revision="í•´ì‹œê°’" ê³ ì •
        )

        my_embedder = Inference(
            "pyannote/embedding",
            device=device,
            use_auth_token=True,
            duration=0.05,   #í•œë²ˆ ì„ë² ë”©ì„ ì¶”ì¶œí•  ì˜¤ë””ì˜¤ ê¸¸ì´(ì´ˆ) - ì‘ì„ ìˆ˜ë¡(ì§§ì€ êµ¬ê°„ ë‹¨ìœ„ ë””í…Œì¼)
            # ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì–¼ë§ˆë‚˜ ì´ë™í• ì§€ - ì‘ì„ ìˆ˜ë¡(ê²¹ì¹¨ ê²½ê³„ ê°ì§€ ì •ë°€) 
            step=0.05        
        )
        # 3. íŒŒì´í”„ë¼ì¸ì— ë®ì–´ì“°ê¸°

        pipeline.embedding = my_embedder


        # *** test_diarize.py ì—ì„œ ì¢‹ì•˜ë˜ íŠœë‹ê°’ ***
        pipeline.segmentation.onset  = 0.25
        pipeline.segmentation.offset = 0.70

        pipeline.min_duration_on  = 0.2
        pipeline.min_duration_off = 0.4
        
        pipeline.clustering.threshold = 0.8

        pipeline.to(device)

        diar = pipeline(
            {"audio": vocal_path},
            min_speakers=min_speakers,
            max_speakers=max_speakers,
        )

    except Exception as e:
        print("âŒ diarization error:", e)
        diar = Annotation()          # ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. Whisper ì„¸ê·¸ë¨¼íŠ¸ì— ë¼ë²¨ ë¶€ì—¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    speaker_segments: dict[str, list] = defaultdict(list)

    for seg in post_word_data:
        best_lbl, best_ov = "unknown", 0.0
        for turn, _, lbl in diar.itertracks(yield_label=True):
            ov = max(0.0, min(turn.end, seg["end"]) - max(turn.start, seg["start"]))
            if ov > best_ov:
                best_ov, best_lbl = ov, lbl
        seg["speaker_label"] = best_lbl
        speaker_segments[best_lbl].append(seg)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ëŒ€ì‚¬ëŸ‰ ê¸°ì¤€ ì£¼ìš” í™”ì ì„ íƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    main_lbl, main_dur = None, 0.0
    for lbl, segs in speaker_segments.items():
        if lbl == "unknown":
            continue
        dur = sum(s["end"] - s["start"] for s in segs)
        if dur > main_dur:
            main_lbl, main_dur = lbl, dur

    if main_lbl is None:
        raise RuntimeError("ì£¼ìš” í™”ì íƒì§€ ì‹¤íŒ¨")

    main_segs = sorted(speaker_segments[main_lbl], key=lambda s: s["start"])
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    return {
        "label":    main_lbl,
        "segments": main_segs,
        "start":    main_segs[0]["start"],
        "end":      main_segs[-1]["end"],
    }
